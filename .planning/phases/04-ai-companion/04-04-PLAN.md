---
phase: 04-ai-companion
plan: "04"
type: execute
wave: 3
depends_on: ["04-01", "04-03"]
files_modified:
  - src/ai/chatResponder.ts
  - src/components/chat/ChatSidebar.tsx
  - src/app/(app)/chat/page.tsx
  - src/components/chat/GlobalChatbar.tsx
autonomous: false
requirements:
  - CHAT-08
  - CTX-05

must_haves:
  truths:
    - "When a user sends a message in drawer, sidebar, or fullscreen, a simulated AI response appears after a short delay"
    - "AI responses feel contextually aware — they reference the article topic if articleContext is set"
    - "Academic profile users receive responses with more depth and a 'Referencias:' section"
    - "General/null profile users receive concise, warm responses"
    - "A typing indicator (animated dots) is visible while the simulated response is being 'generated'"
    - "The user can see and interact with the fully assembled AI companion experience end-to-end"
  artifacts:
    - path: "src/ai/chatResponder.ts"
      provides: "Deterministic mock chat response function with profile-type depth and article context awareness"
      exports: ["getChatResponse", "ChatResponseInput"]
    - path: "src/components/chat/ChatSidebar.tsx"
      provides: "Updated to call getChatResponse and show typing indicator"
      min_lines: 110
    - path: "src/app/(app)/chat/page.tsx"
      provides: "Updated to call getChatResponse and show typing indicator"
      min_lines: 90
  key_links:
    - from: "src/components/chat/ChatSidebar.tsx"
      to: "src/ai/chatResponder.ts"
      via: "getChatResponse called after user message, response added with 800ms delay"
      pattern: "getChatResponse"
    - from: "src/app/(app)/chat/page.tsx"
      to: "src/ai/chatResponder.ts"
      via: "getChatResponse called after user message, same delay pattern"
      pattern: "getChatResponse"
    - from: "src/ai/chatResponder.ts"
      to: "src/lib/chatStore.ts"
      via: "Response typed as ChatMessage, added via addMessage"
      pattern: "ChatMessage|addMessage"
---

<objective>
Wire simulated AI responses into all chat surfaces, making the companion feel intelligent and contextually aware.

Purpose: Without this plan, the chat is just a message input with no responses — a dead end. This plan completes the AI companion loop: user sends a message → typing indicator → simulated response that references the article and adapts to profile depth. This is the final differentiator that makes CULTX a portfolio-ready AI product demo.
Output: chatResponder mock AI function, typing indicator UX, full response wiring in drawer/sidebar/fullscreen. Ends with human verification checkpoint.
</objective>

<execution_context>
@/Users/jccruzb-local/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jccruzb-local/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/ai/contextualRAG.ts
@src/lib/chatStore.ts
@src/components/chat/ChatSidebar.tsx
@src/app/(app)/chat/page.tsx
@src/components/chat/GlobalChatbar.tsx
@src/lib/profile.ts
@src/lib/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create chatResponder mock AI function</name>
  <files>src/ai/chatResponder.ts</files>
  <action>
    Create `src/ai/chatResponder.ts` — pure deterministic mock that returns chat AI responses.

    ```ts
    export interface ChatResponseInput {
      userMessage: string;
      articleContext: string | null;
      profileType: string | null;
      messageCount: number; // how many messages in conversation so far
    }

    export function getChatResponse(input: ChatResponseInput): string
    ```

    Implementation rules:
    - No async, no external calls, pure function
    - Deterministic: `(userMessage.length + messageCount) % responsePool.length` selects the response
    - Response pool: 8-10 pre-written Spanish responses that feel culturally intelligent. Responses MUST feel conversational, warm, editorial (matching CULTX UX tone from CLAUDE.md). Include responses that:
      1. Reference Mexican muralism, Rivera, Orozco, Siqueiros
      2. Reference Mexican architecture (UNAM, Barragan, Legorreta)
      3. Reference pre-Hispanic cultures (Maya, Aztec, Zapotec calendar)
      4. Reference Mexican cinema (Buñuel, Cuarón, del Toro era)
      5. Reference popular music and regional traditions (son jarocho, norteño, cumbia)
      6. Reference contemporary Mexican art scene
      7. Reference Mexican literature (Paz, Fuentes, Poniatowska, Rulfo)
      8. A contextual-sounding fallback that references whatever was in `articleContext`

    - If `articleContext` is set: Response index 8 (fallback) says something like: `"Sobre «${articleContext}»: este tema refleja una de las tensiones más ricas de la cultura mexicana contemporánea — la negociación entre identidad local y narrativa global. ¿Quieres que exploremos algún ángulo específico?"`

    - **Profile depth adaptation:**
      - If `profileType === "Academic"` or `profileType === "Cultural professional"`: append to each response a short "Contexto académico:" paragraph (2-3 sentences more scholarly, with a mock reference). Example: `"\n\nContexto académico: Este fenómeno ha sido estudiado por García Canclini en el marco de las culturas híbridas latinoamericanas (1989). Su análisis sugiere que la modernidad periférica en México genera formas culturales únicas que no pueden reducirse a categorías eurocentristas."`
      - If `profileType === "Student"`: append `"\n\n¿Te gustaría que profundizara en algún aspecto para tu investigación?"` to every response.
      - General / null: no appendage

    - The function is synchronous. The caller simulates delay with `setTimeout`.
  </action>
  <verify>
    Run `npx tsc --noEmit` — zero TypeScript errors.
    Manually call in browser console or test: confirm different messageCount values yield different responses. Confirm Academic profileType adds scholarly context. Confirm articleContext in fallback response.
  </verify>
  <done>
    `getChatResponse({ userMessage: "Hola", articleContext: "El Muralismo", profileType: "Academic", messageCount: 0 })` returns a non-empty Spanish string that ends with an academic context paragraph and "Referencias:"-style citation.
    `getChatResponse({ userMessage: "Hola", articleContext: null, profileType: null, messageCount: 2 })` returns a different response (index 2 in pool) with no academic appendage.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire AI responses and typing indicator into all chat surfaces</name>
  <files>
    src/components/chat/ChatSidebar.tsx
    src/app/(app)/chat/page.tsx
    src/components/chat/GlobalChatbar.tsx
  </files>
  <action>
    Wire `getChatResponse` into all three chat input surfaces. The pattern is identical in each:

    **Shared response trigger pattern** (implement in each surface):
    ```ts
    async function handleSend(inputValue: string) {
      if (!inputValue.trim()) return;
      addMessage({ role: "user", content: inputValue });
      setIsTyping(true);
      await new Promise((r) => setTimeout(r, 800 + Math.random() * 400)); // 800-1200ms delay
      const response = getChatResponse({
        userMessage: inputValue,
        articleContext,
        profileType, // read from getProfile() on component mount
        messageCount: messages.length,
      });
      addMessage({ role: "assistant", content: response });
      setIsTyping(false);
    }
    ```

    Each surface needs:
    - `const [isTyping, setIsTyping] = useState(false)` local state
    - `const [profileType, setProfileType] = useState<string | null>(null)` local state — read from `getProfile()` in a `useEffect` on mount
    - Import `getChatResponse` from `@/ai/chatResponder`
    - Import `getProfile` from `@/lib/profile`

    **Typing indicator component** (inline in each file, or extract to `src/components/chat/TypingIndicator.tsx`):
    - Three animated dots: `● ● ●`
    - Each dot: 6px circle, `backgroundColor: "var(--color-text-muted)"`, border-radius 50%
    - Animation: use CSS keyframes with staggered opacity pulse (0→1→0), 1.2s duration, each dot offset by 0.2s
    - Container: same left-aligned bubble style as assistant messages (`backgroundColor: "var(--color-surface-raised)"`, border-radius, padding 8px 12px)
    - Show when `isTyping === true`, hide when `isTyping === false`
    - For the animation, use a `<style>` tag injected inline or Tailwind's `animate-pulse` class for each dot (staggered via `animationDelay` inline style)

    **ChatSidebar.tsx updates:**
    - Add `isTyping` state and `profileType` state
    - Replace the current naive `addMessage` call in send handler with `handleSend()` function
    - Render typing dots in the messages list when `isTyping === true`, positioned after the last message (before the scroll anchor div)
    - Disable the send button and textarea while `isTyping === true` (add `disabled={isTyping}` to both)

    **chat/page.tsx updates:**
    - Same pattern as ChatSidebar — `isTyping` state, `profileType` state, `handleSend`, typing indicator in messages list, disable input while typing

    **GlobalChatbar.tsx drawer state updates:**
    - Same pattern for the drawer's send button
    - After sending from drawer, call `setChatMode("sidebar")` so user sees the response in the larger surface. This creates a natural UX flow: quick question in drawer → auto-expands to sidebar to show the answer.

    **Note on disabled state:** When `isTyping`, the send button gets `disabled={true}` and opacity 40%. The textarea gets `disabled={true}` with cursor `not-allowed`. This prevents double-sends.

    Keep all existing visual styles. No new token colors or spacing outside the existing token set.
  </action>
  <verify>
    Run `npm run build` — zero errors.
    In drawer: type a message, press Enter → user message appears → typing indicator shows for ~1s → AI response appears below.
    In sidebar: same flow works. Academic profile user sees scholarly appendage in response.
    In fullscreen: same flow works.
    Cannot send a second message while AI is typing (button disabled).
  </verify>
  <done>
    All three chat surfaces (drawer, sidebar, fullscreen) produce simulated AI responses after user messages.
    Typing indicator shows for 800-1200ms before response appears.
    Responses vary by messageCount (not the same response every time).
    Academic/Cultural professional profiles receive responses with academic context paragraphs.
    Student profiles receive responses with a study-oriented follow-up prompt.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Verify complete AI companion experience end-to-end</name>
  <action>Human verification of the complete AI companion experience assembled across plans 04-01 through 04-04. Run `npm run dev` and follow the verification steps below.</action>
  <verify>
    Start the dev server: `npm run dev`
    Navigate to `http://localhost:3000`

    1. Log in (or skip auth if already logged in). Go to Home Feed → click an article.

    2. **Contextual Explore:**
       - Select 10+ characters of article body text
       - Confirm a floating "Explorar" button appears near the selection
       - Click "Explorar" → side panel slides in from the right
       - Confirm: depth label badge (General or Academic), quoted selected text, 2-3 sentence Spanish explanation, 2 related article links
       - Press Escape or click X → panel dismisses
       - Reselect different text → panel reopens with different content

    3. **Send to Chat + Drawer:**
       - Scroll to bottom of article, click "Enviar al chat"
       - Confirm: chatbar expands as drawer with article title shown as context badge
       - Type a question ("Que es el muralismo?"), press Enter
       - Confirm: user message appears, typing indicator shows (~1s), AI response appears

    4. **Sidebar mode:**
       - Click the ArrowsOut icon in drawer → sidebar opens (380px right panel)
       - Confirm article remains visible to the left (not obscured by sidebar)
       - Chat header shows "Discutiendo: [article title]"
       - Type a message in sidebar → AI response appears with typing indicator

    5. **Fullscreen mode:**
       - Click ArrowsOut in sidebar → navigated to /chat fullscreen page
       - Confirm full screen, no article visible
       - Previous messages visible in history
       - Type a message → AI response with typing indicator
       - Click back button → returns to article with sidebar visible

    6. **Collapse:**
       - In sidebar, click ArrowsIn → collapses to minimal pill
       - Pill shows article title in placeholder text

    7. **Academic depth (if possible):**
       - If profile was set to Academic during onboarding, verify AI responses include "Contexto academico:" paragraph with mock citation
  </verify>
  <done>User confirms all 7 verification steps pass. Type "approved" to proceed or describe issues found.</done>
  <resume-signal>Type "approved" if the full AI companion flow works correctly, or describe any issues found.</resume-signal>
</task>

</tasks>

<verification>
Run `npm run build` — zero errors, zero TypeScript errors.
All 15 requirements for Phase 4 (CTX-01 through CTX-06, CHAT-01 through CHAT-09) verifiable end-to-end in browser.
</verification>

<success_criteria>
CHAT-08 satisfied: All chat surfaces produce simulated AI responses with typing indicator.
CTX-05 satisfied: AI responses (both contextual panel and chat) adapt depth to user profile type.
Phase 4 complete: User can select text → Explore panel, open chatbar → drawer → sidebar → fullscreen → back, all with simulated contextually-aware AI responses.
</success_criteria>

<output>
After completion, create `.planning/phases/04-ai-companion/04-04-SUMMARY.md` with what was built, key decisions, and file list.
</output>
